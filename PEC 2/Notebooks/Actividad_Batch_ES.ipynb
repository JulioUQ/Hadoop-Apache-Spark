{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2omGR6IU6wnW",
    "nbgrader": {
     "checksum": "89da8b4d4043133e47cc804f493861a7",
     "grade": false,
     "grade_id": "cell-90505d396cb22993",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **HDFS (Hadoop Distributed File System)**\n",
    "\n",
    "<img src=\"https://hadoop.apache.org/docs/r1.2.1/images/hadoop-logo.jpg\">\n",
    "\n",
    "**HDFS (Hadoop Distributed File System)** es una parte esencial del ecosistema Big Data de Apache Hadoop. HDFS está diseñado para almacenar y gestionar grandes volúmenes de datos distribuidos en varios nodos de un cluster, proporcionando alta tolerancia a fallos y escalabilidad. En este primer ejercicio, vamos a interactuar con HDFS mediante la línea de comandos dentro del entorno de **JupyterLab**, lo que nos permitirá familiarizarnos con las operaciones básicas de este sistema de archivos distribuido.\n",
    "\n",
    "Para comenzar, es necesario abrir un terminal desde **JupyterLab**. Una vez abierto, podemos enviar comandos al sistema de archivos HDFS, que son muy similares a los comandos de bash en entornos Linux. Algunos de los comandos de HDFS que ejecutaremos comenzarán con `hdfs dfs`, seguidos de la operación que deseemos realizar. Por ejemplo, si queremos listar los archivos y directorios en el directorio raíz de HDFS, usaremos el comando ls de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "O4jm7X7h6wnU",
    "nbgrader": {
     "checksum": "3952b915c8d13ff01a662ab9a29c132b",
     "grade": false,
     "grade_id": "cell-762c5996eed99753",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)\n",
    "\n",
    "# Actividad BATCH\n",
    "\n",
    "## Sistema de ficheros HDFS y extracción de conocimiento de fuentes de datos heterogéneas mediante RDDs\n",
    "\n",
    "En esta práctica comenzaremos con una breve introducción a HDFS (Hadoop Distributed File System), para entender cómo se almacena y distribuye la información. Luego, nos meteremos de lleno con Spark RDDs y Spark SQL para procesar grandes volúmenes de datos de forma eficiente. Para finalizar, trabajaremos con datos relacionales y su manejo en entornos distribuidos.\n",
    "\n",
    "### Puntuación de la actividad:\n",
    "- **Ejercicio 1**: Gestión y análisis de datos en HDFS *(0.5 puntos)*\n",
    "- **Ejercicio 2**: Manipulación de RDDs en PySpark *(1.25 puntos)*\n",
    "- **Ejercicio 3**: Análisis de Datos de Tweets en PySpark *(1.25 puntos)*\n",
    "- **Ejercicio 4**: Optimización de Cálculos con Persistencia *(0.25 puntos)*\n",
    "- **Ejercicio 5**: Análisis de Tweets mediante DataFrames y consultas SQL *(2 puntos)*\n",
    "- **Ejercicio 6**: Análisis de Tweets Geolocalizados *(1.5 puntos)*\n",
    "- **Ejercicio 7**: Análisis del Patrón de Actividad Horaria en Twitter *(1 puntos)*\n",
    "- **Ejercicio 8**: Análisis de la Relación entre Tweets y Diputados por Provincia *(0.75 puntos)*\n",
    "- **Ejercicio 9**: Análisis de Interacciones de Retweets y Grados de Usuario *(0.75 puntos)*\n",
    "- **Ejercicio 10**: Distribución del Grado de Salida en una Red de Retweets *(0.75 puntos)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "INHDqlaB6wnX",
    "nbgrader": {
     "checksum": "4e9fb169dd21a71ed1f53cab6e68570f",
     "grade": false,
     "grade_id": "cell-29bd91fdca22294f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "Found 20 items\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-22 16:13 /alluxio\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-21 11:29 /amshbase\n",
      "drwxrwxrwt   - yarn   hadoop          0 2025-10-22 23:46 /app-logs\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-22 12:45 /apps\n",
      "drwxr-xr-x   - yarn   hadoop          0 2025-07-21 11:32 /ats\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-21 11:32 /atsv2\n",
      "drwxr-xr-x   - sgraul hdfs            0 2025-09-24 13:49 /aula_M2.858\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-21 11:32 /bigtop\n",
      "drwxrwxrwx   - flink  hadoop          0 2025-07-22 16:32 /completed-jobs\n",
      "drwxr-xr-x   - yarn   hdfs            0 2025-07-21 14:24 /hadoop\n",
      "drwxr-xr-x   - hbase  hdfs            0 2025-10-22 12:01 /hbase\n",
      "drwx------   - livy   hdfs            0 2025-07-22 16:11 /livy-recovery\n",
      "drwxr-xr-x   - mapred hdfs            0 2025-07-22 14:54 /mapred\n",
      "drwxrwxrwx   - mapred hadoop          0 2025-07-22 15:22 /mr-history\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-21 11:22 /ranger\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-21 11:32 /services\n",
      "drwxrwxrwx   - spark  hadoop          0 2025-11-04 23:00 /spark-history\n",
      "drwxrwxrwx   - hdfs   hdfs            0 2025-10-19 13:31 /tmp\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-10-30 10:54 /user\n",
      "drwxr-xr-x   - hdfs   hdfs            0 2025-07-22 14:53 /warehouse\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fI-BLg-B6wnY",
    "nbgrader": {
     "checksum": "75ea03b0c53e574c854e62c3760fbea7",
     "grade": false,
     "grade_id": "cell-65c81dfb469fa166",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Es importante que todos los comandos se ejecuten correctamente en el entorno **JupyterLab** para obtener los resultados deseados.\n",
    "\n",
    "Para consultar la documentación completa de los comandos disponibles en HDFS, puedes acceder a la guía oficial en el siguiente enlace: [HDFS Command Guide](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html)\n",
    "\n",
    "A lo largo de este ejercicio, utilizaremos algunos de los comandos más comunes de HDFS para realizar operaciones como la creación de directorios, la carga y descarga de archivos, y la gestión de permisos, entre otros. A medida que avanzamos, te familiarizarás con la estructura de HDFS y cómo aprovechar sus funcionalidades en entornos Big Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "K-HGw5r76wnY",
    "nbgrader": {
     "checksum": "efa7eeafb80abf5c9ed8cea31c74070a",
     "grade": false,
     "grade_id": "cell-5549afe0caf22a16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### **Ejercicio 1**: Gestión y análisis de datos en HDFS (*0.5 puntos*)\n",
    "\n",
    "En este ejercicio trabajarás con un conjunto de registros de consumo eléctrico almacenados en el archivo `consumo_hogar_2024.csv`, disponible en la ruta `/aula_M2.858/data/consumo_hogar_2024.csv`.\n",
    "\n",
    "Tu tarea consiste en realizar una exploración completa del archivo directamente en HDFS, verificando su tamaño, permisos, propietario, factor de replicación y número de bloques. Deberás además comprobar que el archivo no presenta errores de integridad ni bloques dañados.\n",
    "\n",
    "Sin descargar el archivo completamente, analiza su estructura y verifica que los datos sean legibles (por ejemplo, revisando encabezados y algunas filas de muestra).\n",
    "\n",
    "Una vez confirmes que el archivo está correcto, crea dentro de tu espacio personal en HDFS una carpeta llamada `procesado` en la ruta `/user/[tu_usuario]/` y reorganiza allí el archivo aplicando un nombre que indique que ha sido validado, por ejemplo `consumo_hogar_2024_validado.csv`.\n",
    "\n",
    "Finalmente, genera un pequeño informe de verificación (en texto plano) que resuma la información principal del archivo (tamaño, bloques, factor de replicación, propietario y fecha del proceso) y guárdalo en la misma carpeta procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "-rw-r--r--   3 martam hdfs        711 2025-10-20 09:28 /aula_M2.858/data/consumo_hogar_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# 1️.1. Verificar información general del archivo\n",
    "!hdfs dfs -ls /aula_M2.858/data/consumo_hogar_2024.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "Connecting to namenode via http://eimtcld3node1:50070/fsck?ugi=jubedaq&files=1&blocks=1&locations=1&path=%2Faula_M2.858%2Fdata%2Fconsumo_hogar_2024.csv\n",
      "FSCK started by jubedaq (auth:SIMPLE) from /172.17.58.200 for path /aula_M2.858/data/consumo_hogar_2024.csv at Tue Nov 04 23:01:47 CET 2025\n",
      "\n",
      "/aula_M2.858/data/consumo_hogar_2024.csv 711 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-1495504404-172.17.58.6-1753086652608:blk_1073760312_19539 len=711 Live_repl=3  [DatanodeInfoWithStorage[172.17.58.200:50010,DS-269ffbda-b619-40ff-8bb8-b8d6d7acfd8a,DISK], DatanodeInfoWithStorage[172.17.58.201:50010,DS-def0af19-c2a5-4be3-bffe-a4a1ace3bfe6,DISK], DatanodeInfoWithStorage[172.17.58.6:50010,DS-94d61690-8bae-419c-8424-c5e44bf76713,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t711 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 711 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t3.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Tue Nov 04 23:01:47 CET 2025 in 2 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/aula_M2.858/data/consumo_hogar_2024.csv' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "# 1.2. Mostrar detalles técnicos (tamaño, factor de replicación, bloques, propietario, etc.)\n",
    "!hdfs fsck /aula_M2.858/data/consumo_hogar_2024.csv -files -blocks -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "Connecting to namenode via http://eimtcld3node1:50070/fsck?ugi=jubedaq&blocks=1&locations=1&racks=1&path=%2Faula_M2.858%2Fdata%2Fconsumo_hogar_2024.csv\n",
      "FSCK started by jubedaq (auth:SIMPLE) from /172.17.58.200 for path /aula_M2.858/data/consumo_hogar_2024.csv at Tue Nov 04 23:01:49 CET 2025\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t711 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 711 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t3.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Tue Nov 04 23:01:49 CET 2025 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/aula_M2.858/data/consumo_hogar_2024.csv' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "# 1.3. Comprobar que no haya bloques dañados\n",
    "!hdfs fsck /aula_M2.858/data/consumo_hogar_2024.csv -blocks -locations -racks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "﻿fecha,hora,id_hogar,consumo_kwh,temperatura_ext,region\n",
      "2024-01-01,00:00,H001,1.23,8.2,Madrid\n",
      "2024-01-01,01:00,H001,1.15,7.9,Madrid\n",
      "2024-01-01,00:00,H002,0.95,9.1,Barcelona\n",
      "2024-01-01,01:00,H002,0.88,8.7,Barcelona\n",
      "2024-01-02,00:00,H001,1.35,7.5,Madrid\n",
      "2024-01-02,01:00,H001,1.20,7.2,Madrid\n",
      "2024-01-02,00:00,H002,1.05,8.3,Barcelona\n",
      "2024-01-02,01:00,H002,0.97,7.8,Barcelona\n",
      "2024-02-01,12:00,H003,2.10,15.2,Sevilla\n"
     ]
    }
   ],
   "source": [
    "# 1.4. Ver una muestra de los datos (sin descargarlo completo)\n",
    "!hdfs dfs -head /aula_M2.858/data/consumo_hogar_2024.csv | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "# 1.5. Crear una carpeta personal “procesado”\n",
    "!hdfs dfs -mkdir -p /user/jubedaq/procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "cp: `/user/jubedaq/procesado/consumo_hogar_2024_validado.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "# 1.6. Copiar y renombrar el archivo validado\n",
    "!hdfs dfs -cp /aula_M2.858/data/consumo_hogar_2024.csv /user/jubedaq/procesado/consumo_hogar_2024_validado.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7. Crear informe de verificación (texto plano) --> Ejecutar en la terminal\n",
    "# !echo \"INFORME DE VERIFICACIÓN - consumo_hogar_2024.csv\n",
    "# ------------------------------------------------------\n",
    "# Propietario original: martam\n",
    "# Verificado por: jubedaq\n",
    "# Tamaño: 711 bytes\n",
    "# Factor de replicación: 3\n",
    "# Número de bloques: 1\n",
    "# Estado del sistema de archivos: HEALTHY (sin bloques dañados)\n",
    "# Número de data-nodes: 3\n",
    "# Número de racks: 1\n",
    "# Ruta origen: /aula_M2.858/data/consumo_hogar_2024.csv\n",
    "# Ruta destino: /user/jubedaq/procesado/consumo_hogar_2024_validado.csv\n",
    "# Fecha de validación: Sun Nov 02 12:32:52 CET 2025\n",
    "# ------------------------------------------------------\n",
    "# Observaciones:\n",
    "# El archivo fue leído correctamente. La estructura CSV contiene cabecera y registros con\n",
    "# campos: fecha, hora, id_hogar, consumo_kwh, temperatura_ext, region.\n",
    "# No se detectaron errores de integridad ni bloques corruptos.\n",
    "# ------------------------------------------------------\" > informe_validacion.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "id": "k7WYuwSh6wnZ",
    "nbgrader": {
     "checksum": "86e7480fa473517874c91c04486848ee",
     "grade": true,
     "grade_id": "cell-7e25613cec2ad2b5",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "# 1.8. Subir el informe al HDFS\n",
    "!hdfs dfs -put -f informe_validacion.txt /user/jubedaq/procesado/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "Found 2 items\n",
      "-rw-r--r--   3 jubedaq students        711 2025-11-02 12:34 /user/jubedaq/procesado/consumo_hogar_2024_validado.csv\n",
      "-rw-r--r--   3 jubedaq students        309 2025-11-04 23:02 /user/jubedaq/procesado/informe_validacion.txt\n"
     ]
    }
   ],
   "source": [
    "# 1.9. Verificar que el informe se subio a la carpeta HDFS de mi usuario\n",
    "!hdfs dfs -ls /user/jubedaq/procesado/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/bigtop/3.3.0/usr/lib/tez/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "INFORME DE VERIFICACIÓN - consumo_hogar_2024.csv\\n---------------------------------------\\nPropietario: jubedaq\\nTamaño: [indicado por fsck]\\nFactor de replicación: [indicado por fsck]\\nBloques: [indicado por fsck]\\nFecha de validación: dg. de nov.  2 12:37:29 CET 2025\\nEstado: OK (sin bloques dañados)\n"
     ]
    }
   ],
   "source": [
    "# 1.10 Verificacion visual del contenido del informe\n",
    "!hdfs dfs -cat /user/jubedaq/procesado/informe_validacion.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "csXz8gNH6wnZ",
    "nbgrader": {
     "checksum": "0fb5cd8c1f7754d237805c82c5c08b79",
     "grade": false,
     "grade_id": "cell-668bdd296b8f16d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **Apache Spark RDDs (Resilient Distributed Datasets)**\n",
    "\n",
    "En el marco del procesamiento de grandes volúmenes de datos con Apache Spark, los RDDs, o Resilient Distributed Datasets, juegan un papel fundamental. Un RDD es una colección de elementos que se distribuyen a través de un clúster de nodos y sobre la cual se pueden aplicar operaciones que se ejecutan en paralelo.\n",
    "\n",
    "Recordemos sus características:\n",
    "\n",
    "- Inmutabilidad: Una vez que se crea un RDD, no se puede modificar. En lugar de eso, cualquier operación que modifique los datos generará un nuevo RDD.\n",
    "\n",
    "- Distribución: Los RDDs están repartidos entre los diferentes nodos del clúster, permitiendo un procesamiento paralelo eficiente.\n",
    "\n",
    "- Tolerancia a Fallos: Los RDDs son resistentes a fallos. En caso de que un nodo falle, Spark puede reconstruir los datos perdidos a partir de los datos originales y las operaciones realizadas.\n",
    "\n",
    "Esta estructura permite un procesamiento eficiente y escalable de datos, lo que es esencial para trabajar con grandes volúmenes de información en entornos de clúster.\n",
    "\n",
    "A continuación se muestra el código que debéis ejecutar para configurar vuestro entorno de Spark.\n",
    "\n",
    "> Como referencia a todos métodos que se requieren para implementar esta práctica podéis consultar:\n",
    "> * [API Python de Spark](https://archive.apache.org/dist/spark/docs/2.4.0/api/python/index.html)\n",
    "\n",
    "### Configuración del entorno python + spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8D9I5JZs6wnZ",
    "nbgrader": {
     "checksum": "5a775efbfaef2bb2723199a314158114",
     "grade": false,
     "grade_id": "cell-0e19edd4cce1b7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/04 23:02:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "ActividadSparkSQL_usuario\n",
      "3.3.4\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "SPARK_HOME_PATH = \"/usr/bigtop/current/spark-client/\" \n",
    "os.environ['SPARK_HOME'] = SPARK_HOME_PATH\n",
    "findspark.init(SPARK_HOME_PATH)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ActividadRDDs_usuario\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.appName)\n",
    "print(spark.version)\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-Jb7To_Y6wna",
    "nbgrader": {
     "checksum": "e2ff80f94a6e0769eb776417825f00c6",
     "grade": false,
     "grade_id": "cell-c908488b1840d49a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### **Ejercicio 2**: Manipulación de RDDs en PySpark (*1.25 puntos*)\n",
    "\n",
    "En este ejercicio, te proporcionamos dos listas de números en las que realizarás diversas operaciones sobre ellas utilizando RDDs en PySpark. La solución y el enfoque quedan a tu criterio.\n",
    "Contexto:\n",
    "\n",
    "Tienes dos listas de números que representan datos de sensores:\n",
    "- **Sensor A**: Números del 1 al 25.\n",
    "- **Sensor B**: Números del 15 al 35.\n",
    "\n",
    "Debes crear RDDs a partir de las listas de números de cada sensor. Una vez hecho esto, para el **Sensor A**, transforma cada número en una tupla `(número, número al cubo)`. Y filtra solo aquellos números cuyo cubo sea **múltiplo de 7** y **mayor que 50**. El RDD resultante se almacenará en una variable llamada `rdd_a_filtrado`. Finalmente, agrupa los números filtrados según si son **pares, impares o múltiplos de 5** (un número puede pertenecer a más de un grupo), y guarda este resultado en `rdd_a_grupos`.\n",
    "\n",
    "Volviendo a los RDDs iniciales, calcula la intersección entre los RDDs de **Sensor A y Sensor B**, guárdalo en `rdd_interseccion` y calcula la diferencia de **Sensor B menos Sensor A**, que se guardará en `rdd_diferencia`. A continuación, realiza una unión de ambos RDDs, eliminando los valores duplicados y ordénala de mayor a menor, guardando el resultado en `rdd_union`.\n",
    "\n",
    "- Imprime los resultados de cada una de las operaciones realizadas utilizando el método `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "id": "U4-tbS-F6wna",
    "nbgrader": {
     "checksum": "ebee90310d5e3fbeb127429820dcc0dd",
     "grade": false,
     "grade_id": "cell-816167e349b9878c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 2.1. Crear listas base (datos de sensores)\n",
    "sensor_a = list(range(1, 26))   # Números del 1 al 25\n",
    "sensor_b = list(range(15, 36))  # Números del 15 al 35\n",
    "\n",
    "# Crear RDDs a partir de las listas\n",
    "rdd_a = sc.parallelize(sensor_a)\n",
    "rdd_b = sc.parallelize(sensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2. Transformar RDD A en tuplas (n, n^3)\n",
    "rdd_a_tuplas = rdd_a.map(lambda x: (x, x**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD A Filtrado (número, cubo):\n",
      "[(7, 343), (14, 2744), (21, 9261)]\n"
     ]
    }
   ],
   "source": [
    "# 2.3. Filtrar aquellos cuyo cubo sea múltiplo de 7 y mayor que 50\n",
    "rdd_a_filtrado = rdd_a_tuplas.filter(lambda x: x[1] % 7 == 0 and x[1] > 50)\n",
    "\n",
    "print(\"\\nRDD A Filtrado (número, cubo):\")\n",
    "print(rdd_a_filtrado.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD A Grupos:\n",
      "[('impar', [7, 21]), ('par', [14])]\n"
     ]
    }
   ],
   "source": [
    "# 2.4. Agrupar los números filtrados por tipo: par, impar, múltiplo de 5\n",
    "def clasificar_numero(n):\n",
    "    grupos = []\n",
    "    if n % 2 == 0:\n",
    "        grupos.append((\"par\", n))\n",
    "    if n % 2 != 0:\n",
    "        grupos.append((\"impar\", n))\n",
    "    if n % 5 == 0:\n",
    "        grupos.append((\"multiplo_5\", n))\n",
    "    return grupos\n",
    "\n",
    "rdd_a_grupos = rdd_a_filtrado.flatMap(lambda x: clasificar_numero(x[0])) \\\n",
    "                             .groupByKey() \\\n",
    "                             .mapValues(list)\n",
    "\n",
    "print(\"\\nRDD A Grupos:\")\n",
    "print(rdd_a_grupos.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD Intersección:\n",
      "[16, 18, 20, 22, 24, 15, 17, 19, 21, 23, 25]\n"
     ]
    }
   ],
   "source": [
    "# 2.5. Intersección entre Sensor A y Sensor B\n",
    "rdd_interseccion = rdd_a.intersection(rdd_b)\n",
    "print(\"\\nRDD Intersección:\")\n",
    "print(rdd_interseccion.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD Diferencia (B - A):\n",
      "[26, 28, 30, 32, 34, 27, 29, 31, 33, 35]\n"
     ]
    }
   ],
   "source": [
    "# 2.6. Diferencia (Sensor B - Sensor A)\n",
    "rdd_diferencia = rdd_b.subtract(rdd_a)\n",
    "print(\"\\nRDD Diferencia (B - A):\")\n",
    "print(rdd_diferencia.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD Unión (sin duplicados, ordenada desc):\n",
      "[35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# 2.7. Unión sin duplicados y ordenada de mayor a menor\n",
    "rdd_union = rdd_a.union(rdd_b).distinct().sortBy(lambda x: -x)\n",
    "print(\"\\nRDD Unión (sin duplicados, ordenada desc):\")\n",
    "print(rdd_union.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fvNnx-Ul6wnb",
    "nbgrader": {
     "checksum": "119b92212657ac0237e00d5c2414c75a",
     "grade": true,
     "grade_id": "cell-a85f72b6a72bc941",
     "locked": true,
     "points": 1.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eOCYrOSl6wnb",
    "nbgrader": {
     "checksum": "de8639a973e7adf2172f36c66cf16d99",
     "grade": false,
     "grade_id": "cell-af3f41fdb008b797",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### **Ejercicio 3**: Análisis de Datos de Tweets en PySpark (*1.25 puntos*)\n",
    "\n",
    "En este ejercicio, trabajarás con un archivo JSON llamado `tweets_sample.json` que se encuentra en la ruta `/aula_M2.858/data/tweets_sample.json`. Este archivo contiene datos de tweets y métricas relacionadas. Deberás utilizar PySpark para realizar un análisis de los datos. La estructura del archivo JSON incluye información como el número de retweets, likes, seguidores, y más. Sin embargo, para este ejercicio, te enfocarás en procesar y analizar el contenido textual de los tweets.\n",
    "\n",
    "- Carga el archivo JSON en un RDD utilizando el método `textFile()`. Examina la estructura de los datos para identificar cómo extraer el contenido relevante.\n",
    "\n",
    "- Extrae el campo tweets de cada uno de los tweets. Define y aplica una función para limpiar el texto. Esta función debe eliminar la puntuación, convertir el texto a minúsculas y asegurar que haya un solo espacio entre las palabras.\n",
    "\n",
    "- Divide el texto en palabras y filtra las palabras para quedarte con aquellas que tengan menos de 7 caracteres. Después, realiza un conteo de palabras distintas, guárdalo en la variable `palabras_distintas_rdd`.\n",
    "\n",
    "- Por último, encuentra las 5 palabras más frecuentes que terminen en vocal. Guárdalo en la variable `top_5_palabras`.\n",
    "\n",
    "- Imprime los resultados de cada una de las operaciones realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación de carga (4 primeras líneas):\n",
      "{\"tweet_id\": 1, \"user\": \"usuario1\", \"followers\": 150, \"retweets\": 5, \"likes\": 10, \"tweet\": \"¡Hola mundo! Este es un tweet de prueba para ver cómo funciona. #prueba #mundo\"}\n",
      "{\"tweet_id\": 2, \"user\": \"usuario2\", \"followers\": 300, \"retweets\": 2, \"likes\": 7, \"tweet\": \"Los datos son el nuevo petróleo. Analiza, visualiza y actúa. #data #análisis\"}\n",
      "{\"tweet_id\": 3, \"user\": \"usuario3\", \"followers\": 500, \"retweets\": 15, \"likes\": 20, \"tweet\": \"Un día productivo en la oficina. ¿Alguna vez has tenido un día así? #productividad\"}\n",
      "{\"tweet_id\": 4, \"user\": \"usuario4\", \"followers\": 250, \"retweets\": 10, \"likes\": 5, \"tweet\": \"¿Sabías que Python es uno de los lenguajes de programación más utilizados? #Python #programación\"}\n"
     ]
    }
   ],
   "source": [
    "# 3.1. Cargar el archivo JSON como RDD\n",
    "tweets_rdd = sc.textFile(\"/aula_M2.858/data/tweets_sample.json\")\n",
    "\n",
    "# Verificar que se ha cargado correctamente (mostrar 4 líneas)\n",
    "print(\"\\nVerificación de carga (4 primeras líneas):\")\n",
    "for linea in tweets_rdd.take(4):\n",
    "    print(linea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación de extracción (5 tweets):\n",
      "- ¡Hola mundo! Este es un tweet de prueba para ver cómo funciona. #prueba #mundo\n",
      "- Los datos son el nuevo petróleo. Analiza, visualiza y actúa. #data #análisis\n",
      "- Un día productivo en la oficina. ¿Alguna vez has tenido un día así? #productividad\n",
      "- ¿Sabías que Python es uno de los lenguajes de programación más utilizados? #Python #programación\n",
      "- La programación puede ser divertida y emocionante. ¡No te rindas! #programación\n"
     ]
    }
   ],
   "source": [
    "# 3.2. Extraer el campo 'tweet' \n",
    "import json # Necesario para trabajar con ficheros json\n",
    "def extraer_texto(linea):\n",
    "    try:\n",
    "        data = json.loads(linea)\n",
    "        return data.get(\"tweet\", \"\")\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "tweets_texto_rdd = tweets_rdd.map(extraer_texto).filter(lambda x: x != \"\")\n",
    "\n",
    "# Verificación: mostrar 5 tweets procesados correctamente\n",
    "print(\"\\nVerificación de extracción (5 tweets):\")\n",
    "for t in tweets_texto_rdd.take(5):\n",
    "    print(\"-\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación de limpieza (5 tweets):\n",
      "- hola mundo este es un tweet de prueba para ver cómo funciona prueba mundo\n",
      "- los datos son el nuevo petróleo analiza visualiza y actúa data análisis\n",
      "- un día productivo en la oficina alguna vez has tenido un día así productividad\n",
      "- sabías que python es uno de los lenguajes de programación más utilizados python programación\n",
      "- la programación puede ser divertida y emocionante no te rindas programación\n"
     ]
    }
   ],
   "source": [
    "# 3.3. Limpiar el texto: minúsculas, sin puntuación, espacios simples\n",
    "import re # Necesaria para procesar texto mediante patrones\n",
    "\n",
    "def limpiar_texto(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"[^a-záéíóúüñ0-9\\s]\", \" \", t)  # reemplaza todo lo que no sea letra, número o espacio por un espacio\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()         # colapsa varios espacios consecutivos en uno solo\n",
    "    return t\n",
    "\n",
    "tweets_limpios_rdd = tweets_texto_rdd.map(limpiar_texto)\n",
    "\n",
    "# Verificación: mostrar 5 tweets limpios\n",
    "print(\"\\nVerificación de limpieza (5 tweets):\")\n",
    "for t in tweets_limpios_rdd.take(5):\n",
    "    print(\"-\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación de división y filtrado (20 palabras):\n",
      "['hola', 'mundo', 'este', 'es', 'un', 'tweet', 'de', 'prueba', 'para', 'ver', 'cómo', 'prueba', 'mundo', 'los', 'datos', 'son', 'el', 'nuevo', 'y', 'actúa']\n"
     ]
    }
   ],
   "source": [
    "# 3.4. Dividir en palabras y filtrar las de menos de 7 caracteres\n",
    "palabras_rdd = tweets_limpios_rdd.flatMap(lambda t: t.split(\" \"))\n",
    "palabras_filtradas_rdd = palabras_rdd.filter(lambda p: len(p) < 7 and p != \"\")\n",
    "\n",
    "# Verificación: mostrar 20 palabras para comprobar\n",
    "print(\"\\nVerificación de división y filtrado (20 palabras):\")\n",
    "print(palabras_filtradas_rdd.take(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras distintas (muestra):\n",
      "['hola', 'mundo', 'este', 'es', 'un', 'tweet', 'de', 'prueba', 'para', 'ver']\n"
     ]
    }
   ],
   "source": [
    "# 3.5. Contar palabras distintas\n",
    "palabras_distintas_rdd = palabras_filtradas_rdd.distinct()\n",
    "print(\"\\nPalabras distintas (muestra):\")\n",
    "print(palabras_distintas_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "id": "xCN50DIe6wnb",
    "nbgrader": {
     "checksum": "06f14d71e43160b8f049941447133df4",
     "grade": false,
     "grade_id": "cell-e8558308f2480e66",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 palabras más frecuentes que terminan en vocal:\n",
      "[('de', 5), ('la', 4), ('para', 3), ('mundo', 2), ('prueba', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 3.6. Encontrar las 5 palabras más frecuentes que terminen en vocal\n",
    "def termina_en_vocal(p):\n",
    "    return len(p) > 0 and p[-1] in \"aeiouáéíóú\"\n",
    "\n",
    "palabras_vocal_rdd = palabras_filtradas_rdd.filter(termina_en_vocal)\n",
    "top_5_palabras = (palabras_vocal_rdd\n",
    "                  .map(lambda p: (p, 1))\n",
    "                  .reduceByKey(lambda a, b: a + b)\n",
    "                  .sortBy(lambda x: -x[1])\n",
    "                  .take(5))\n",
    "\n",
    "print(\"\\nTop 5 palabras más frecuentes que terminan en vocal:\")\n",
    "print(top_5_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Kcz9wG7W6wnc",
    "nbgrader": {
     "checksum": "13929bc95c3a0879fbb581ddab71b5fc",
     "grade": true,
     "grade_id": "cell-b0116f4101cd8691",
     "locked": true,
     "points": 1.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BUnS9Of46wnc",
    "nbgrader": {
     "checksum": "af5286e1027e4ae481ee3348bc1bc28e",
     "grade": false,
     "grade_id": "cell-3a2e9a75c09972fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### **Ejercicio 4**: Optimización de Cálculos con Persistencia (*0.25 puntos*)\n",
    "\n",
    "Para reducir los tiempos de ejecución en Spark, es fundamental utilizar la persistencia de un RDD mediante el método `persist()`. Esta técnica es particularmente útil cuando se realizan múltiples operaciones repetidas sobre un mismo RDD.\n",
    "\n",
    "Cuando persistes un RDD, Spark almacena los datos en memoria (o en disco, dependiendo del nivel de persistencia, para ver mas sobre los niveles de persistencia ir a la web [Persistencia Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)) para evitar recomputaciones cada vez que se necesita realizar una acción sobre el RDD. Esto significa que cada nodo del clúster guarda en su memoria las particiones del RDD que ha procesado, permitiendo que las siguientes operaciones sobre el RDD sean mucho más rápidas.\n",
    "\n",
    "**Medición de Rendimiento**\n",
    "\n",
    "Para medir la mejora en los tiempos de ejecución, podemos utilizar la función mágica `%%time` en un entorno Jupyter Notebook, que permite observar:\n",
    "\n",
    "- Wall clock time: Tiempo total real que lleva ejecutar una tarea, incluyendo la CPU, el tiempo de entrada/salida (I/O), y las posibles comunicaciones entre nodos en el clúster.\n",
    "\n",
    "- CPU time: Tiempo efectivo en que la CPU está ocupada ejecutando la tarea, excluyendo otras latencias como la de entrada/salida.\n",
    "\n",
    "En este ejercicio, se explorará el uso de la persistencia en RDDs (Resilient Distributed Datasets) utilizando PySpark. El objetivo es observar cómo la persistencia afecta al rendimiento de las operaciones de transformación y acción sobre los RDDs.\n",
    "\n",
    "- Crea un RDD a partir de una lista de números que va del 1 al 10.000.\n",
    "\n",
    "- Filtra el RDD para obtener solo los números mayores a 5.000 y almacena este resultado en un nuevo RDD.\n",
    "\n",
    "- Aplica una transformación para duplicar los valores del RDD filtrado y guárdalo en un nuevo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "id": "dpEgB3u06wnc",
    "nbgrader": {
     "checksum": "ee543905ca11f95dbceb47ca6fe43a8b",
     "grade": true,
     "grade_id": "cell-45ba7d8ba8695b7f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 4.1. Crear el RDD con numeros del 1 al 10 000\n",
    "rdd_numeros = sc.parallelize(range(1, 10001))\n",
    "\n",
    "# 4.2. Filtrar los números mayores a 5000\n",
    "rdd_filtrado = rdd_numeros.filter(lambda x: x > 5000)\n",
    "\n",
    "# 4.3. Transformar (duplicar los valores)\n",
    "rdd_duplicado = rdd_filtrado.map(lambda x: (x, x * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DBTNm7l96wnc",
    "nbgrader": {
     "checksum": "d30a38125db95c829ee0a3de2999f9ba",
     "grade": false,
     "grade_id": "cell-07cc8324a3e0fcd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Utiliza el método collect() para recuperar y mostrar los números mayores a 5.000 y sus dobles, y mide el tiempo que tarda en ejecutarse esta operación utilizando la función mágica `%%time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "id": "qqEizinN6wnc",
    "nbgrader": {
     "checksum": "2ad976349fd917fd446c59ae92aaef71",
     "grade": true,
     "grade_id": "cell-6891d054f7cb3677",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin persistencia (10 primeros registros): [(5001, 10002), (5002, 10004), (5003, 10006), (5004, 10008), (5005, 10010), (5006, 10012), (5007, 10014), (5008, 10016), (5009, 10018), (5010, 10020)]\n",
      "CPU times: user 9.79 ms, sys: 3.2 ms, total: 13 ms\n",
      "Wall time: 92.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4.4. Ejecutar sin persistencia\n",
    "resultado_sin_persistencia = rdd_duplicado.collect()\n",
    "\n",
    "# Mostrar los primeros 10 resultados\n",
    "print(\"Sin persistencia (10 primeros registros):\", resultado_sin_persistencia[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TZ9HI1Y06wnd",
    "nbgrader": {
     "checksum": "dcaa573dfbc6fb0cb8be0988cc5f477d",
     "grade": false,
     "grade_id": "cell-b309d0ab3f0c21cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Aplica la persistencia sobre el RDD de números mayores a 5.000 para que su contenido se mantenga en memoria entre las operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "id": "h7eNSXle6wnd",
    "nbgrader": {
     "checksum": "0d5189360010c9dda915deb58c155f42",
     "grade": true,
     "grade_id": "cell-be900f5c2f58e86d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[171] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# 4.5.Aplicar persistencia\n",
    "rdd_filtrado.persist(StorageLevel.MEMORY_ONLY) # Opcion más eficiente para la CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J051G-f46wnd",
    "nbgrader": {
     "checksum": "35714fd41daaa6df0468ba1ebc580c9f",
     "grade": false,
     "grade_id": "cell-b2ebe6a6ee538053",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Vuelve a ejecutar el método collect() como antes. Compara este tiempo con el tiempo de la primera ejecución. (Puedes ejecutarlo varias veces y ver que ocurre con el tiempo de procesamiento.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "id": "UHZjtIy16wnd",
    "nbgrader": {
     "checksum": "542511bfe74da76a934b81e11a42ae50",
     "grade": true,
     "grade_id": "cell-aeda08145665dfd6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 4.6. Recalculo la transformacion sobre el RDD persistido\n",
    "rdd_doblado_persistido = rdd_filtrado.map(lambda x: (x, x * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con persistencia (10 primeros registros) [(5001, 10002), (5002, 10004), (5003, 10006), (5004, 10008), (5005, 10010), (5006, 10012), (5007, 10014), (5008, 10016), (5009, 10018), (5010, 10020)]\n",
      "CPU times: user 6.74 ms, sys: 2.69 ms, total: 9.43 ms\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultado_con_persistencia = rdd_doblado_persistido.collect()\n",
    "\n",
    "# Mostrar los primeros 10 resultados\n",
    "print(\"Con persistencia (10 primeros registros)\",  resultado_con_persistencia[:10]) ## Con cada ejecución el Wall time disminuye hasta llegar a un umbral en el que varia muy poco hacia arriba y hacia abajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a0NY0mQN6wnd",
    "nbgrader": {
     "checksum": "52e1202971424c872d70e1848af43f01",
     "grade": false,
     "grade_id": "cell-9b3ab8245642d014",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Deshazte de la persistencia del RDD utilizando unpersist() para liberar recursos y detén la sesión de Spark al final del ejercicio con sc.stop()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "id": "WXqSQIHM6wnd",
    "nbgrader": {
     "checksum": "8e6641189fcca3d6b6d0b05326778595",
     "grade": true,
     "grade_id": "cell-272df1b106dbe53b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 4.7. Libero la memoria\n",
    "rdd_filtrado.unpersist()\n",
    "\n",
    "# Detener spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UgDOcR3w6wne",
    "nbgrader": {
     "checksum": "2c11322b2a2b4a1e0f196bc01f0103e7",
     "grade": false,
     "grade_id": "cell-b7a79085ff3f6d60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Al terminar el ejercicio, analiza y comenta los resultados obtenidos, explicando cómo la persistencia afectó el rendimiento de tus cálculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "id": "wbpjQWoE6wne",
    "nbgrader": {
     "checksum": "2ab03cfad1a3232c0e68bff89070bf55",
     "grade": true,
     "grade_id": "cell-0b0cd9b99abc4b91",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Analisis y comentario de los resultados obtenidos:\n",
    "#> Tras aplicar persistencia, el tiempo total de ejecución (Wall time) se ha reducido. \n",
    "#> Esto se debe a que Spark evita recalcular el RDD filtrado en memoria y reutiliza los datos cacheados. \n",
    "#> Además, en ejecuciones repetidas el tiempo mejora aún más, evidenciando el beneficio de la persistencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hjdFAnex6wnf",
    "nbgrader": {
     "checksum": "f25dfa3c038ede79aee93eae70531260",
     "grade": false,
     "grade_id": "cell-6941fca55160a2ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **Apache Spark Dataframes**\n",
    "\n",
    "En esta parte de la práctica vamos a introducir los elementos que ofrece Spark para trabajar con estructuras de datos. Veremos desde estructuras muy simples hasta estructuras complejas, donde los campos pueden a su vez tener campos anidados. En concreto utilizaremos datos de Twitter capturados en el contexto de las elecciones generales en España del 28 de abril de 2019\n",
    "\n",
    "### Configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t547oQsZ6wng",
    "nbgrader": {
     "checksum": "39844f2f84f538dfdecbd8bb66d37835",
     "grade": false,
     "grade_id": "cell-3a82c2c4742a2717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActividadSparkSQL_usuario\n",
      "3.3.4\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "SPARK_HOME_PATH = \"/usr/bigtop/current/spark-client/\" \n",
    "os.environ['SPARK_HOME'] = SPARK_HOME_PATH\n",
    "findspark.init(SPARK_HOME_PATH)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ActividadSparkSQL_usuario\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "#    .config(\"spark.hadoop.hive.execution.engine\", \"mr\") \\\n",
    "#    .enableHiveSupport() \\\n",
    "\n",
    "print(spark.sparkContext.appName)\n",
    "print(spark.version)\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2Sp-sJIx6wng",
    "nbgrader": {
     "checksum": "0531ceee4755437a031ce7f87702e7ae",
     "grade": false,
     "grade_id": "cell-4af720440341a897",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from math import floor\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "o9LqCrZ96wnh",
    "nbgrader": {
     "checksum": "617d63356ce5f4787f4bf1751d99c11b",
     "grade": false,
     "grade_id": "cell-e50df8d283bd63e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--jars /opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar pyspark-shell\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ydxodc736wnh",
    "nbgrader": {
     "checksum": "f11146e348acef8d1007f605f382949b",
     "grade": false,
     "grade_id": "cell-7b57297d05fc4a31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introducción a dataframes estructurados y operaciones sobre ellos\n",
    "\n",
    "Como ya se ha mencionado, en los siguientes ejercicis vamos a utilizar datos de Twitter que recolectamos durante las elecciones generales en España del 28 de abril de 2019. Como veremos, los tweets tienen una estructura interna bastante compleja que hemos simplificado un poco en esta práctica.\n",
    "\n",
    "Lo primero que vamos a aprender es cómo importar este tipo de datos a nuestro entorno. Uno de los tipos de archivos más comunes para guardar este formato de información es [la estructura JSON](https://en.wikipedia.org/wiki/JSON). Esta estructura permite guardar información en un texto plano de diferentes objetos siguiendo una estructura de diccionario donde cada campo tiene asignado una clave y un valor. La estructura puede ser anidada, o sea que una clave puede tener como valor otra estructura de tipo diccionario.\n",
    "\n",
    "Spark SQL permite leer datos de muchos formatos diferentes. Se os pide que [leáis el fichero JSON](https://spark.apache.org/docs/2.4.0/sql-data-sources-json.html) de la ruta ```/aula_M2.858/data/tweets28a_sample.json```. Este archivo contiene un pequeño *sample*, un 0.1% de la base de datos completa (en un siguiente apartado veremos cómo realizar este *sampleado*). En esta ocasión no se os pide especificar la estructura del dataframe ya que la función de lectura la inferirá automáticamente.\n",
    "\n",
    "**Ejemplo de lectura (Rellenar con lo correspondiente para la lectura del archivo json)**:\n",
    "\n",
    "```Python\n",
    "tweets_sample = spark.read.json(<FILL IN>)\n",
    "\n",
    "print(\"Loaded dataset contains %d tweets\" % tweets_sample.count())\n",
    "```\n",
    "\n",
    "Para mostrar la estructura del dataset que acabamos de cargar, podéis obtener la información acerca de cómo está estructurado el DataTable utilizando el método ```printSchema()```. Tenéis que familiarizaros con esta estructura ya que será la que utilizaremos durante los próximos ejercicios. Recordad también que no todos los tweets tienen todos los campos, como por ejemplo la ubicación (campo ```place```). Cuando esto pasa el campo pasa a ser ```NULL```. Podéis ver más información sobre este tipo de datos en [este enlace](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object).\n",
    "\n",
    "Ahora debéis introducir el ejemplo de lectura con el `<FILL IN>` relleno según corresponda para la lectura del archivo JSON. Y, a continuación, mostraréis la estructura del dataset utilizando `printSchema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "id": "uefxFSv16wnh",
    "nbgrader": {
     "checksum": "d65a2113d708b4c6a95bdb0036ac2385",
     "grade": true,
     "grade_id": "cell-ce4290b281b39fcc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset cargado contiene 27268 tweets.\n",
      "\n",
      "Con las siguientes columnas ['_id', 'created_at', 'lang', 'place', 'retweeted_status', 'text', 'user'].\n",
      "\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset JSON \n",
    "tweets_sample = spark.read.json(\"/aula_M2.858/data/tweets28a_sample.json\")\n",
    "\n",
    "# Mostrar número total de tweets cargados\n",
    "print(f\"El dataset cargado contiene {tweets_sample.count()} tweets.\\n\") \n",
    "print(f\"Con las siguientes columnas {tweets_sample.columns}.\\n\") \n",
    "\n",
    "# Mostrar la estructura del dataset\n",
    "tweets_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oP5Oe4ZZ6wni",
    "nbgrader": {
     "checksum": "e842d30dd44e2eada4fc279e28423fe7",
     "grade": false,
     "grade_id": "cell-0cab68f38ef4bbb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "assert tweets_sample.count() == 27268, \"Incorrect answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YytKfI3t6wni",
    "nbgrader": {
     "checksum": "41e5826ca374e34df4f38f770eb0d84b",
     "grade": false,
     "grade_id": "cell-1cc2e7d7bf9ac262",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Queries sobre dataframes complejos\n",
    "\n",
    "A continuación vamos a ver como realizar consultas sobre el dataset de los tweets. Vamos a utilizar [sentencias *SQL*](https://www.w3schools.com/sql/default.asp) (como las utilizadas en la mayoría de bases de datos relacionales).\n",
    "\n",
    "Lo primero que se debe hacer es registrar el dataframe de tweets como una tabla de SQL. Para ello utilizaremos [sqlContext.registerDataFrameAsTable()](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerDataFrameAsTable). Para ejecutar comandos sql solo teneis que utilizar el metodo sql() del objecto contexto, en este caso `sqlContext`.\n",
    "\n",
    "#### Queries a través del pipeline\n",
    "\n",
    "Las tablas de Spark SQL ofrecen otro mecanismo para aplicar las transformaciones y obtener resultados similares a los que se obtendría aplicando una consulta SQL. Por ejemplo, utilizando el siguiente pipeline obtendremos el texto de todos los tweets en español:\n",
    "\n",
    "```\n",
    "tweets_sample.where(\"lang == 'es'\").select(\"text\")\n",
    "```\n",
    "\n",
    "Que es equivalente a la siguiente sentencia SQL:\n",
    "\n",
    "```\n",
    "SELECT text\n",
    "FROM tweets_sample\n",
    "WHERE lang == 'es'\n",
    "```\n",
    "\n",
    "Podéis consultar el [API de spark SQL](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html) para encontrar más información sobre como utilizar las diferentes transformaciones en tablas.\n",
    "\n",
    "### **Ejercicio 5**: Análisis de Tweets mediante DataFrames y consultas SQL (*2 puntos*)\n",
    "\n",
    "Anteriormente ya has realizado la lectura del conjunto `tweets28a_sample.json` en formato JSON. Ahora deberás asegúrate de registrar el DataFrame como una tabla SQL llamada `tweets_sample`.\n",
    "\n",
    "***Nota:*** Debido a que es posible que ejecutes estas líneas de codigo várias veces, vamos a tomar la precaución de ejecutar el comando sql para eliminar tablas antes de que las crees, ya que puede existir la posibilidad de que ya existan.\n",
    "\n",
    "`spark.sql(\"DROP TABLE IF EXISTS tweets_sample\")`\n",
    "\n",
    "A continuación, se pide crear una tabla y registrarla con el nombre ```users_agg``` con [la información agregada](https://www.w3schools.com/sql/sql_groupby.asp) de los usuarios que tengan definido su idioma (```user.lang```) como español (```es```). En concreto se pide que la tabla contenga las siguientes columnas:\n",
    "- **screen_name:** nombre del usuario\n",
    "- **friends_count:** número máximo (ver nota) de personas a las que sigue\n",
    "- **tweets:** número de tweets realizados\n",
    "- **followers_count:** número máximo (ver nota) personas que siguen al usuario.\n",
    "\n",
    "El orden en el cual se deben mostrar los registros es orden descendente acorde al número de tweets.\n",
    "\n",
    "***Nota:*** Es importante que te fijes que el nombre de *friends* y *followers* puede diferir a lo largo de la adquisición de datos. En este caso vamos a utilizar la función de agregación `MAX` sobre cada uno de estos campos para evitar segmentar el usuario en diversas instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "id": "Qe4wB35c6wni",
    "nbgrader": {
     "checksum": "27c9a3f26c00d0beb76135f100ebe56a",
     "grade": false,
     "grade_id": "cell-c8f53fb28c6e4279",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-04T23:02:17,276 INFO [Thread-4] org.apache.hadoop.hive.conf.HiveConf - Found configuration file file:/usr/bigtop/current/hive-client/conf/hive-site.xml\n",
      "2025-11-04T23:02:17,391 WARN [Thread-4] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.heapsize does not exist\n",
      "2025-11-04T23:02:17,391 WARN [Thread-4] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.hook.proto.base-directory does not exist\n",
      "2025-11-04T23:02:17,391 WARN [Thread-4] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.strict.managed.tables does not exist\n",
      "2025-11-04T23:02:17,392 WARN [Thread-4] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.fetch.partition.stats does not exist\n",
      "25/11/04 23:02:17 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "2025-11-04T23:02:17,438 INFO [Thread-4] SessionState - Hive Session ID = 77871476-b8fe-432f-8d0e-25f8bbd2ef87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 77871476-b8fe-432f-8d0e-25f8bbd2ef87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-04T23:02:17,585 INFO [Thread-4] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://eimtcld3node2:9083\n",
      "2025-11-04T23:02:17,610 INFO [Thread-4] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 1\n",
      "2025-11-04T23:02:17,643 INFO [Thread-4] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.\n",
      "2025-11-04T23:02:17,643 INFO [Thread-4] org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=jubedaq (auth:SIMPLE) retries=24 delay=5 lifetime=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==========================================>                (5 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset users_agg contiene 17925 tweets.\n",
      "\n",
      "Con las siguientes columnas ['screen_name', 'friends_count', 'tweets', 'followers_count'].\n",
      "\n",
      "root\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- friends_count: long (nullable = true)\n",
      " |-- tweets: long (nullable = false)\n",
      " |-- followers_count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 5.1 \n",
    "# Eliminar tabla si ya existe\n",
    "spark.sql(\"DROP TABLE IF EXISTS tweets_sample\")\n",
    "\n",
    "# Registrar el DataFrame como tabla SQL\n",
    "tweets_sample.createOrReplaceTempView(\"tweets_sample\")\n",
    "\n",
    "# Crear tabla agregada con usuarios en español\n",
    "users_agg = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        user.screen_name AS screen_name,\n",
    "        MAX(user.friends_count) AS friends_count,\n",
    "        COUNT(*) AS tweets,\n",
    "        MAX(user.followers_count) AS followers_count\n",
    "    FROM tweets_sample\n",
    "    WHERE user.lang = 'es'\n",
    "    GROUP BY user.screen_name\n",
    "    ORDER BY tweets DESC\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar número total de tweets cargados\n",
    "print(f\"El dataset users_agg contiene {users_agg.count()} tweets.\\n\") \n",
    "print(f\"Con las siguientes columnas {users_agg.columns}.\\n\") \n",
    "\n",
    "# Mostrar la estructura del dataset\n",
    "users_agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9AoI2NZ46wni",
    "nbgrader": {
     "checksum": "6c5944cb6649e315c611fde0c2d8fe16",
     "grade": true,
     "grade_id": "cell-2a9bc81f180704ed",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MZgjvZwU6wnj",
    "nbgrader": {
     "checksum": "6070d045e9d2582099df07150bdb7ea8",
     "grade": false,
     "grade_id": "cell-37543a51e22733d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A continuación recurriremos al [JOIN de tablas](https://www.w3schools.com/sql/sql_join.asp) para combinar la información entre tablas. Debes combinar la tabla `users_agg` y la tabla `tweets_sample` utilizando un `INNER JOIN` para obtener una nueva tabla con el nombre retweeted con la siguiente información:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***friends_count:*** número máximo de personas a las que sigue\n",
    "- ***followers_count:*** número máximo de personas que siguen al usuario.\n",
    "- ***tweets:*** número de tweets realizados por el usuario.\n",
    "- ***retweeted:*** número de retweets obtenidos por el usuario.\n",
    "- ***ratio_tweet_retweeted:*** ratio de retweets por número de tweets publicados $\\frac{retweets}{tweets}$\n",
    "\n",
    "La tabla resultante tiene que estar ordenada de manera descendente según el valor de la columna `ratio_tweet_retweeted`.\n",
    "\n",
    "Por último, utilizando queries a través de pipeline, debes crear una tabla `user_retweets` a partir de la tabla `tweets_sample`, utilizando transformaciones que contenga dos columnas:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***retweeted:*** número de retweets\n",
    "\n",
    "Ordena la tabla en orden descendente utilizando el valor de la columna ```retweeted```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "id": "qFEbvUNx6wnj",
    "nbgrader": {
     "checksum": "349a59052e3acd94cf095c464cda708d",
     "grade": false,
     "grade_id": "cell-a32533af184a42ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla retweeted contiene 1081 registros.\n",
      "\n",
      "Con las siguientes columnas: ['screen_name', 'friends_count', 'followers_count', 'tweets', 'retweeted', 'ratio_tweet_retweeted'].\n",
      "\n",
      "root\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- friends_count: long (nullable = true)\n",
      " |-- followers_count: long (nullable = true)\n",
      " |-- tweets: long (nullable = false)\n",
      " |-- retweeted: long (nullable = false)\n",
      " |-- ratio_tweet_retweeted: double (nullable = true)\n",
      "\n",
      "\n",
      "Primeras filas de la tabla retweeted:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "|   screen_name|friends_count|followers_count|tweets|retweeted|ratio_tweet_retweeted|\n",
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "|          PSOE|        13635|         671073|     1|      155|                155.0|\n",
      "|  CiudadanosCs|        92910|         511896|     1|      117|                117.0|\n",
      "|     JuntsXCat|          202|          88515|     1|       73|                 73.0|\n",
      "|  PartidoPACMA|         1498|         232932|     1|       63|                 63.0|\n",
      "|  pablocasado_|         4567|         238926|     1|       50|                 50.0|\n",
      "|voxnoticias_es|         2146|          29582|     1|       44|                 44.0|\n",
      "|RaiLopezCalvet|         7579|          13574|     1|       43|                 43.0|\n",
      "|        iunida|        10225|         558318|     1|       39|                 39.0|\n",
      "|        Xuxipc|          311|         184967|     1|       37|                 37.0|\n",
      "|       Panik81|         1587|          15374|     1|       29|                 29.0|\n",
      "+--------------+-------------+---------------+------+---------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.2.1.\n",
    "# Eliminar tabla si ya existe\n",
    "spark.sql(\"DROP TABLE IF EXISTS users_agg\")\n",
    "\n",
    "# Registrar users_agg como tabla SQL\n",
    "users_agg.createOrReplaceTempView(\"users_agg\")\n",
    "\n",
    "# Crear tabla retweeted mediante INNER JOIN\n",
    "retweeted = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ua.screen_name,\n",
    "        ua.friends_count,\n",
    "        ua.followers_count,\n",
    "        ua.tweets,\n",
    "        COUNT(*) AS retweeted,\n",
    "        COUNT(*) / ua.tweets AS ratio_tweet_retweeted\n",
    "    FROM users_agg ua\n",
    "        INNER JOIN tweets_sample ts ON ua.screen_name = ts.retweeted_status.user.screen_name\n",
    "    GROUP BY \n",
    "        ua.screen_name, \n",
    "        ua.friends_count, \n",
    "        ua.followers_count, \n",
    "        ua.tweets\n",
    "    ORDER BY ratio_tweet_retweeted DESC\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar información de la tabla\n",
    "print(f\"La tabla retweeted contiene {retweeted.count()} registros.\\n\")\n",
    "print(f\"Con las siguientes columnas: {retweeted.columns}.\\n\")\n",
    "retweeted.printSchema()\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "print(\"\\nPrimeras filas de la tabla retweeted:\")\n",
    "retweeted.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla user_retweets contiene 7664 registros.\n",
      "\n",
      "Con las siguientes columnas: ['screen_name', 'retweeted'].\n",
      "\n",
      "root\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- retweeted: long (nullable = false)\n",
      "\n",
      "Primeras filas de la tabla user_retweets:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   screen_name|retweeted|\n",
      "+--------------+---------+\n",
      "|        vox_es|      299|\n",
      "| Santi_ABASCAL|      238|\n",
      "|  ahorapodemos|      238|\n",
      "|      iescolar|      166|\n",
      "| AlbanoDante76|      161|\n",
      "|          PSOE|      155|\n",
      "|AntonioMaestre|      154|\n",
      "|          KRLS|      149|\n",
      "|        boye_g|      142|\n",
      "|  CiudadanosCs|      117|\n",
      "+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 5.2.2.\n",
    "# Crear tabla user_retweets mediante transformaciones con pipeline\n",
    "user_retweets = (tweets_sample\n",
    "    .filter(tweets_sample.retweeted_status.isNotNull())  # Filtrar solo tweets con retweets\n",
    "    .select(\"retweeted_status.user.screen_name\")         # Seleccionar el screen_name\n",
    "    .groupBy(\"screen_name\")                              # Agrupar por usuario\n",
    "    .count()                                             # Contar retweets\n",
    "    .withColumnRenamed(\"count\", \"retweeted\")             # Renombrar columna\n",
    "    .orderBy(\"retweeted\", ascending=False)               # Ordenar descendente\n",
    ")\n",
    "\n",
    "# Mostrar información de la tabla\n",
    "print(f\"La tabla user_retweets contiene {user_retweets.count()} registros.\\n\")\n",
    "print(f\"Con las siguientes columnas: {user_retweets.columns}.\\n\")\n",
    "user_retweets.printSchema()\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "print(\"Primeras filas de la tabla user_retweets:\")\n",
    "user_retweets.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Gz3R771f6wno",
    "nbgrader": {
     "checksum": "0cf3144e969bdbaf93667773efb56246",
     "grade": true,
     "grade_id": "cell-7cb8718b88310c41",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gzMf1kwQ6wnp",
    "nbgrader": {
     "checksum": "2ec53981534d4b659fd65ffe24e9d4cb",
     "grade": false,
     "grade_id": "cell-1a287b320c41f881",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Bases de datos HIVE y operaciones complejas\n",
    "\n",
    "Hasta ahora hemos estado trabajando con un pequeño sample de los tweets generados (el 0.1%). En esta parte de la actividad vamos a ver como trabajar y tratar con el dataset completo. Para ello vamos a utilizar tanto transformaciones sobre tablas como operaciones sobre RDD cuando sea necesario.\n",
    "\n",
    "Es importante tener en cuenta que muchas veces los datos con los que trabajamos se utilizarán en diversos proyectos. En lugar de manejar directamente los archivos, es más eficiente y organizado recurrir a una base de datos para gestionar la información. En el ecosistema de Hadoop, una de las bases de datos más utilizadas es [Apache Hive](https://hive.apache.org/). Sin embargo, es crucial entender que Hive no es una base de datos convencional. En realidad, funciona como un metastore que mapea archivos en el sistema de archivos distribuido de Hadoop (HDFS).\n",
    "\n",
    "Esto significa que Hive no almacena los datos en su propio formato de base de datos, sino que actúa como una interfaz que permite a los usuarios ejecutar consultas SQL sobre los datos almacenados en HDFS. Esto proporciona una forma eficiente de acceder y manipular grandes volúmenes de datos distribuidos sin necesidad de moverlos o convertirlos a un formato tradicional de base de datos.\n",
    "\n",
    "La manera de acceder a esta base de datos es tal y como se muestra en el siguiente código (debéis ejecutarlo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0472ffe29c52a0cef94aa088ffeee9d",
     "grade": false,
     "grade_id": "cell-163fd1faeda71a4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------+\n",
      "|     Nombre_Tabla|Base_Datos|Es_Temporal|\n",
      "+-----------------+----------+-----------+\n",
      "|        boxscores|   default|      false|\n",
      "|    boxscores_ext|   default|      false|\n",
      "|boxscores_interna|   default|      false|\n",
      "|boxscores_managed|   default|      false|\n",
      "|    boxscores_orc|   default|      false|\n",
      "|   boxscores_parq|   default|      false|\n",
      "|    ext_boxscores|   default|      false|\n",
      "|        ext_games|   default|      false|\n",
      "|     ext_injuries|   default|      false|\n",
      "|      ext_players|   default|      false|\n",
      "|        ext_teams|   default|      false|\n",
      "|            games|   default|      false|\n",
      "|        games_ext|   default|      false|\n",
      "|    games_interna|   default|      false|\n",
      "|    games_managed|   default|      false|\n",
      "|        games_orc|   default|      false|\n",
      "|       games_parq|   default|      false|\n",
      "|         injuries|   default|      false|\n",
      "|     injuries_ext|   default|      false|\n",
      "| injuries_interna|   default|      false|\n",
      "+-----------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Obtener la lista de objetos 'Table'\n",
    "tables_list = spark.catalog.listTables()\n",
    "\n",
    "# 2. Convertir la lista de objetos en una lista de tuplas (Nombre, BD, Temporal)\n",
    "# Utilizamos una comprensión de lista (list comprehension)\n",
    "data_for_df = [(t.name, t.database, t.isTemporary) for t in tables_list]\n",
    "\n",
    "# 3. Definir el esquema manualmente para evitar cualquier inferencia automática\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "manual_schema = StructType([\n",
    "    StructField(\"Nombre_Tabla\", StringType(), True),\n",
    "    StructField(\"Base_Datos\", StringType(), True),\n",
    "    StructField(\"Es_Temporal\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# 4. Crear el DataFrame con los datos y el esquema definido\n",
    "tables_df = spark.createDataFrame(data_for_df, schema=manual_schema)\n",
    "\n",
    "# 5. Mostrar el resultado\n",
    "tables_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EggncoXw6wnp",
    "nbgrader": {
     "checksum": "6e7a78e032b600c8f3cdbf62354a66b8",
     "grade": false,
     "grade_id": "cell-ae4372ff1d253b60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Más allá de las transformaciones SQL\n",
    "\n",
    "Algunas veces vamos a necesitar obtener resultados que precisan operaciones que van más allá de lo que podemos conseguir (cómodamente) utilizando el lenguaje SQL. En esta parte de la práctica vamos a practicar cómo pasar de una tabla a un RDD, para hacer operaciones complejas, y luego volver a pasar a una tabla.\n",
    "\n",
    "Ahora viene la parte interesante. Una tabla puede convertirse en un RDD a través del atributo ```.rdd```. Este atributo guarda la información de la tabla en una lista donde cada elemento es un [objeto del tipo ```Row```](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Row). Los objetos pertenecientes a esta clase pueden verse como diccionarios donde la información de las diferentes columnas queda reflejada en forma de atributo. Por ejemplo, imaginad que tenemos una tabla con dos columnas, nombre y apellido, si utilizamos el atributo ```.rdd``` de dicha tabla obtendremos una lista con objetos del tipo row donde cada objeto tiene dos atributos: nombre y apellido. Para acceder a los atributos solo tenéis que utilizar la sintaxis *punto* de Python, e.g., ```row.nombre``` o ```row.apellido```.\n",
    "\n",
    "### **Ejercicio 6**: Análisis de Tweets Geolocalizados (*1.5 puntos*)\n",
    "\n",
    "Dada la tabla de tweets `tweets28a_sample25`, debes crear una variable `tweets` utilizando el objeto `spark` y el método `table()`. Utilizando una sentencia SQL, se requiere extraer información sobre los tweets que contienen datos geolocalizados (es decir, aquellos donde el campo `place` no es nulo) y determinar cuántos tweets se han generado desde cada lugar. Los resultados deben ser presentados en orden descendente por la cantidad de tweets.\n",
    "\n",
    "**Esquema sentencia sql**\n",
    "```Python\n",
    "tweets_place = spark.sql(<FILL IN>)\n",
    "```\n",
    "\n",
    "A continuación, crea una tabla llamada `tweets_place` que contenga dos columnas:\n",
    "\n",
    "- ***name:*** nombre del lugar desde donde se ha generado el tweet.\n",
    "- ***tweets:*** número total de tweets realizados en dicho lugar.\n",
    "\n",
    "Finalmente, muestra los 10 lugares con mayor número de tweets en la tabla resultante.\n",
    "\n",
    "Adicionalmente, crea una tabla llamada `tweets_geo` que contenga únicamente los tweets que tienen información de geolocalización, y asegúrate de que incluya el nombre del lugar. A partir de esta tabla, crea un objeto ```tweets_place_rdd``` que contenga una lista de tuplas con la información ```(name, tweets)``` sobre el nombre del lugar y el número de tweets generados desde allí.\n",
    "\n",
    "Una vez generado este RDD vamos a crear una tabla. El primer paso es generar por cada tupla un objeto Row que contenga un atributo ```name``` y un atributo ```tweets```. Ahora solo tenéis que aplicar el método ```toDF()``` para generar una tabla. Ordenad las filas de esta tabla por el número de tweets en orden descendente.\n",
    "\n",
    "El ejercicio deberá realizarse combinando tanto sentencias SQL como RDD en Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bccfe24a7883d8df01642b781653b82d",
     "grade": false,
     "grade_id": "cell-1e2dc0fc3e7f5532",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla tweets contiene 6354961 tweets.\n",
      "\n",
      "Con las siguientes columnas: ['_id', 'created_at', 'lang', 'place', 'retweeted_status', 'text', 'user'].\n",
      "\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n",
      "Primeras filas de la tabla tweets:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+-----+--------------------+--------------------+--------------------+\n",
      "|                _id|         created_at|lang|place|    retweeted_status|                text|                user|\n",
      "+-------------------+-------------------+----+-----+--------------------+--------------------+--------------------+\n",
      "|1117169839657844736|2019-04-13 22:57:00|  en| null|{1117009066402955...|RT @DearAuntCrabb...|{329, 871, 547969...|\n",
      "|1117169840802934787|2019-04-13 22:57:00|  es| null|{1117037292458262...|RT @PAH_Burgos: #...|{18, 17, 88521307...|\n",
      "|1117169840870100994|2019-04-13 22:57:00|  es| null|{1116682791503192...|RT @ViajesFalcon:...|{1270, 356, 76286...|\n",
      "|1117169840886882304|2019-04-13 22:57:00|  es| null|{1117157909568397...|RT @AiniApgg: Est...|{3887, 3860, 6100...|\n",
      "|1117169841985683456|2019-04-13 22:57:00|  es| null|{1117083000343232...|RT @Santi_ABASCAL...|{137, 593, 289042...|\n",
      "|1117169842329653248|2019-04-13 22:57:01|  es| null|{1116442625911992...|RT @CarmenPicazoC...|{111, 84, 3063577...|\n",
      "|1117169844984610817|2019-04-13 22:57:01|  es| null|                null|@Earco1977 Pues a...|{5893, 4783, 5090...|\n",
      "|1117169845798305792|2019-04-13 22:57:01|  es| null|{1117108350993539...|RT @FrayJosepho: ...|{3834, 1582, 6116...|\n",
      "|1117169846037426176|2019-04-13 22:57:01|  es| null|{1116961638589059...|RT @FiloPolitics:...|{243, 136, 836025...|\n",
      "|1117169846272315392|2019-04-13 22:57:02| und| null|                null|@CastigadorY @jus...|{1900, 1898, 2573...|\n",
      "+-------------------+-------------------+----+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 6.1.\n",
    "# Cargar la tabla tweets28a_sample25 en una variable\n",
    "tweets = spark.table(\"tweets28a_sample25\")\n",
    "\n",
    "# Mostrar información de la tabla\n",
    "print(f\"La tabla tweets contiene {tweets.count()} tweets.\\n\")\n",
    "print(f\"Con las siguientes columnas: {tweets.columns}.\\n\")\n",
    "tweets.printSchema()\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "print(\"Primeras filas de la tabla tweets:\")\n",
    "tweets.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla tweets_place contiene 4038 lugares únicos.\n",
      "\n",
      "Con las siguientes columnas: ['name', 'tweets'].\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- tweets: long (nullable = false)\n",
      "\n",
      "Top 10 lugares con más tweets:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid|  4911|\n",
      "|  Barcelona|  3481|\n",
      "|    Sevilla|   959|\n",
      "|   Valencia|   689|\n",
      "|   Zaragoza|   597|\n",
      "|Villamartín|   570|\n",
      "|     Málaga|   546|\n",
      "|     Murcia|   461|\n",
      "|      Palma|   416|\n",
      "|   Alicante|   407|\n",
      "+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La tabla tweets_geo contiene 44477 tweets geolocalizados.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 10 tuplas del RDD (nombre lugar, tweets):\n",
      "\n",
      "La tabla tweets_place_from_rdd contiene 4038 lugares únicos.\n",
      "\n",
      "Con las siguientes columnas: ['name', 'tweets'].\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- tweets: long (nullable = true)\n",
      "\n",
      "Top 10 lugares con más tweets (creado desde RDD):\n",
      "\n",
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid|  4911|\n",
      "|  Barcelona|  3481|\n",
      "|    Sevilla|   959|\n",
      "|   Valencia|   689|\n",
      "|   Zaragoza|   597|\n",
      "|Villamartín|   570|\n",
      "|     Málaga|   546|\n",
      "|     Murcia|   461|\n",
      "|      Palma|   416|\n",
      "|   Alicante|   407|\n",
      "+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.2.\n",
    "# Consulta SQL para extraer lugares con tweets geolocalizados\n",
    "tweets_place = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        place.name as name,\n",
    "        COUNT(*) AS tweets\n",
    "    FROM tweets28a_sample25\n",
    "    WHERE place IS NOT NULL\n",
    "    GROUP BY place.name\n",
    "    ORDER BY tweets DESC\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar información de la tabla \n",
    "print(f\"La tabla tweets_place contiene {tweets_place.count()} lugares únicos.\\n\")\n",
    "print(f\"Con las siguientes columnas: {tweets_place.columns}.\\n\")\n",
    "tweets_place.printSchema()\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "print(\"Top 10 lugares con más tweets:\")\n",
    "tweets_place.show(10)\n",
    "\n",
    "# 6.3.\n",
    "# Crear tabla con tweets que tienen geolocalizacion\n",
    "tweets_geo = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM tweets28a_sample25\n",
    "    WHERE place IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar información de la tabla \n",
    "print(f\"\\nLa tabla tweets_geo contiene {tweets_geo.count()} tweets geolocalizados.\\n\")\n",
    "\n",
    "# 6.4.\n",
    "# Convertir la tabla tweets_geo a rdd\n",
    "tweets_place_rdd = (tweets_geo\n",
    "    .select(\"place.name\")                    # Selecciono el nombre del lugar\n",
    "    .rdd                                     # Convierto a rdd\n",
    "    .map(lambda row: (row.name, 1))          # Crear tuplas 'nombre lugar' + 1\n",
    "    .reduceByKey(lambda a, b: a + b)         # Sumar conteos por lugar\n",
    "    .sortBy(lambda x: x[1], ascending=False) # Ordenar descendente\n",
    ")\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "print(\"Primeras 10 tuplas del RDD (nombre lugar, tweets):\\n\")\n",
    "tweets_place_rdd.take(10)\n",
    "\n",
    "# 6.5.\n",
    "# Crear objetos Row a partir de las tuplas\n",
    "tweets_place_from_rdd = (tweets_place_rdd\n",
    "    .map(lambda x: Row(name = x[0], tweets = x[1])) # Tupla a Row\n",
    "    .toDF()\n",
    "    .orderBy(\"tweets\", ascending=False)\n",
    ")\n",
    "\n",
    "# Mostrar información de la tabla \n",
    "print(f\"La tabla tweets_place_from_rdd contiene {tweets_place_from_rdd.count()} lugares únicos.\\n\")\n",
    "print(f\"Con las siguientes columnas: {tweets_place_from_rdd.columns}.\\n\")\n",
    "tweets_place_from_rdd.printSchema()\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "print(\"Top 10 lugares con más tweets (creado desde RDD):\\n\")\n",
    "tweets_place_from_rdd.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zG-MgMGM6wnp",
    "nbgrader": {
     "checksum": "55be938148d03daab4a46535925be3a4",
     "grade": true,
     "grade_id": "cell-78674ee06e771c5f",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6dPuOQey6wnp",
    "nbgrader": {
     "checksum": "5698ce56432fbf09eb8b4bea272b97d7",
     "grade": false,
     "grade_id": "cell-23a0b12ac13b214f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sampling\n",
    "\n",
    "En muchas ocasiones, antes de lanzar costosos procesos, es una práctica habitual tratar con un pequeño conjunto de los datos para investigar algunas propiedades o simplemente para depurar nuestros algoritmos, a esta tarea se la llama sampling. En esta parte de la práctica vamos a ver los dos principales métodos de sampling y cómo utilizarlos.\n",
    "\n",
    "### Nota:\n",
    "Para producir un gráfico de barras utilizando [Pandas](https://pandas.pydata.org/) donde se muestre la información que acabáis de generar. Primero transformad la tabla a pandas utilizando el método `toPandas()`. Plotead la tabla resultante utilizando [la funcionalidad gráfica de pandas.](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.bar.html)\n",
    "\n",
    "### Homogéneo\n",
    "\n",
    "El primer sampling que vamos a ver es [el homogeneo](https://en.wikipedia.org/wiki/Simple_random_sample). Este sampling se basta en simplemente escoger una fracción de la población seleccionando aleatoriamente elementos de esta.\n",
    "\n",
    "Primero de todo vamos a realizar un sampling homogéneo del 1% de los tweets generados en periodo electoral sin reemplazo. Guardad en una variable ```tweets_sample``` este sampling utilizando el método ```sample``` descrito en la [API de pyspark SQL](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html). El seed que vais a utilizar para inicializar el generador aleatorio es 42.\n",
    "\n",
    "**Esquema**\n",
    "```Python\n",
    "seed = 42\n",
    "fraction = 0.01\n",
    "\n",
    "tweets_sample = tweets.<FILL IN>\n",
    "\n",
    "print(\"Number of tweets sampled: {0}\".format(tweets_sample.count()))\n",
    "```\n",
    "\n",
    "### **Ejercicio 7**: Análisis del Patrón de Actividad Horaria en Twitter (*1 puntos*)\n",
    "A partir de una muestra del 1% de los tweets disponibles, se desea analizar el patrón de uso diario de Twitter, prestando especial atención a la actividad horaria. El objetivo es calcular y visualizar el promedio de tweets generados en cada hora del día. Para ello se debe crear una tabla ```tweets_timestamp``` con la información:\n",
    "- ***created_at***: timestamp de cuando se publicó el tweet.\n",
    "- ***hour***: a que hora del dia corresponde.\n",
    "- ***day***: Fecha en formato MM-dd-YY\n",
    "\n",
    "La tabla tiene que estar en orden ascendente según la columna `created_at`\n",
    "\n",
    "**Pista**: Para crear las columnas \"hour\" y \"day\" en tu tabla tweets_timestamp, puedes utilizar withColumn(). La función ```hour``` os servirá para extraer la hora del timestamp y la función ```date_format``` os permitirá generar la fecha.\n",
    "\n",
    "A continuación, utiliza la muestra de tweets para extraer la hora y fecha de publicación, y a partir de esa información, determina cuántos tweets se generan por hora. Asegúrate de ajustar el promedio de tweets para reflejar lo que ocurriría en el conjunto completo de datos y presenta los resultados en un gráfico de barras que muestre la actividad horaria promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "qtzL_7L36wnq",
    "nbgrader": {
     "checksum": "d42ffcba30c4275c72a3898441cf11a8",
     "grade": false,
     "grade_id": "cell-01457685517d0638",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OgDgWBFP6wnq",
    "nbgrader": {
     "checksum": "9954dfd466918eec8e07d3ddd14f84f8",
     "grade": true,
     "grade_id": "cell-fb88c4554401a21c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CvtJQBLt6wnq",
    "nbgrader": {
     "checksum": "a96c884d1986ffdb9493e7b1aeb1193e",
     "grade": false,
     "grade_id": "cell-6ff6bcc2d64bf97d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Estratificado\n",
    "\n",
    "En muchas ocasiones el sampling homogéneo no es adecuado ya que por la propia estructura de los datos determinados segmentos pueden estar sobre-representadas. Este es el caso que observamos en los tweets donde las grandes áreas urbanas están sobrerepresentadas si lo comparamos con el volumen de población. En esta actividad vamos a ver cómo aplicar esta técnica al dataset de tweets, para obtener un sampling que respete la proporción de diputados por provincia.\n",
    "\n",
    "En España, el proceso electoral asigna un volumen de diputados a cada provincia que depende de la población y de un porcentaje mínimo asignado por ley. En el contexto Hive que hemos creado previamente (```spark```) podemos encontrar una tabla (```province_28a```) que contiene información sobre las circunscripciones electorales. Cargad ésta tabla en una variable con nombre ```province```.  Cargad esta tabla en una variable con nombre ```province```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Avyeqr7w6wnr",
    "nbgrader": {
     "checksum": "0c49f7e93a6286d9195c12b3027a691f",
     "grade": false,
     "grade_id": "cell-729e1550eb8962b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "province.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yaVp-6hY6wnr",
    "nbgrader": {
     "checksum": "637e2869195e5cfc10c7e381b388f427",
     "grade": true,
     "grade_id": "cell-f1cd83a975d207eb",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert province.count() == 52, \"Incorrect answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hd5HkBol6wnr",
    "nbgrader": {
     "checksum": "1b7d7201e0d2d74733c1bf5ea9facd44",
     "grade": false,
     "grade_id": "cell-f9911970f4c63098",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Para hacer un sampling estratificado lo primero que tenemos que hacer es determinar la fracción que queremos asignar a cada categoría. En este caso queremos una fracción que haga que la ratio tweets diputado sea igual para todas las capitales de provincia. Debemos tener en cuenta que la precisión de la geolocalización en Twitter es normalmente a nivel de ciudad. Por eso, para evitar incrementar la complejidad del ejercicio, vamos a utilizar los tweets en capitales de provincia como proxy de los tweets en toda la provincia.\n",
    "\n",
    "### **Ejercicio 8**: Análisis de la Relación entre Tweets y Diputados por Provincia (*0.75 puntos*)\n",
    "\n",
    "Lo primero que tenéis que hacer es crear un tabla ```info_tweets_province``` que debe contener:\n",
    "- ***capital:*** nombre de la capital de provincia.\n",
    "- ***tweets:*** número de tweets geolocalizados en cada capital\n",
    "- ***diputados:*** diputados que asignados a la provincia.\n",
    "- ***ratio_tweets_diputado:*** número de tweets por diputado.\n",
    "\n",
    "Debéis ordenar la lista por ```ratio_tweets_diputado``` en orden ascendente.\n",
    "\n",
    "***Nota:*** Podéis realizar este ejercicio de muchas maneras, probablemente la más fácil es utilizar la tabla ```tweets_place``` que habéis generado en el ejercicio 5. Recordad cómo utilizar el ```join()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "xTwAGEEY6wnr",
    "nbgrader": {
     "checksum": "b115142d8de3c7e165b4a9d2e6277d6f",
     "grade": false,
     "grade_id": "cell-c0fc9c41ad719792",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I1SmX1wP6wnr",
    "nbgrader": {
     "checksum": "8018dce8516f111be08c9c69babe9107",
     "grade": false,
     "grade_id": "cell-3579f2ae4e162dae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# VARIABLES DADAS\n",
    "output = info_tweets_province.first()\n",
    "maximum_ratio = floor(output.ratio_tweets_diputado * 100) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c4kkciGU6wnr",
    "nbgrader": {
     "checksum": "44046e0a51c076552b37fe2d9f43907c",
     "grade": true,
     "grade_id": "cell-ce2d2c783e22bc49",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "92ONIgbO6wns",
    "nbgrader": {
     "checksum": "08fa8f2c7fb72783091608e02ff8fd44",
     "grade": false,
     "grade_id": "cell-1ea74ea5851742ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A continuación, vamos a necesitar es un diccionario con nombre ```ratios``` donde cada capital de provincia es una llave y su valor asociado es la fracción de tweets que vamos a samplear. En este caso lo que queremos es que la ratio de tweets por cada diputado sea similar para cada capital de provincia.\n",
    "\n",
    "Como queremos que el sampling sea lo más grande posible y no queremos que ninguna capital este infrarepresentada el ratio de tweets por diputado será el valor más pequeño podéis observar en la tabla ```info_tweets_province```, que corresponde a 11.66 tweets por diputado en Teruel. Tenéis este valor guardado en la variable ```maximum_ratio```.\n",
    "\n",
    "*Nota:* El método ```collectAsMap()``` transforma un PairRDD en un diccionario.\n",
    "\n",
    "Por último, genera una tabla ```geo_tweets``` con todos los tweets geolocalizados. Ahora ya estamos en disposición de hacer el sampling estratificado por población. Para ello podéis utilizar el método ```sampleBy()```. Utilizad 42 como seed del generador pseudoaleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "EJZdRzGS6wns",
    "nbgrader": {
     "checksum": "b5b65b8f5b0f7aa9252505b6ffab3c3a",
     "grade": false,
     "grade_id": "cell-819a133ac1379113",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4HbOGFpB6wns",
    "nbgrader": {
     "checksum": "e567761130a4340dcc703549002b1b40",
     "grade": true,
     "grade_id": "cell-098cab856fea0075",
     "locked": true,
     "points": 0.35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "B2IO9nOx6wns",
    "nbgrader": {
     "checksum": "0e8e5ccfa6c5836f3a2604c337e80779",
     "grade": false,
     "grade_id": "cell-01d153466f9deacc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introducción a los datos relacionales\n",
    "\n",
    "El hecho de trabajar con una base de datos que contiene información generada en una red social nos permite introducir el concepto de datos relacionales. Podemos definir datos relacionales como aquellos en los que existen relaciones entre las entidades que constituyen la base de datos. Si estas relaciones son binarias, relaciones 1 a 1, podemos representar las relaciones como un grafo compuesto por un conjunto de vértices $\\mathcal{V}$ y un conjunto de aristas $\\mathcal{E}$ que los relacionan.\n",
    "\n",
    "En el caso de grafos que emergen de manera orgánica, este tipo de estructura va más allá de los grafos regulares que seguramente conocéis. Este tipo de estructuras se conocen como [redes complejas](https://es.wikipedia.org/wiki/Red_compleja). El estudio de la estructura y dinámicas de este tipo de redes ha contribuido a importantes resultados en campos tan dispares como la física, la sociología, la ecología o la medicina.\n",
    "\n",
    "![complex_network](https://images.squarespace-cdn.com/content/5150aec6e4b0e340ec52710a/1364574727391-XVOFAB9P6GHKTDAH6QTA/lastfm_800_graph_white.png?content-type=image%2Fpng)\n",
    "\n",
    "En esta última parte de la práctica vamos a trabajar con este tipo de datos. En concreto vamos a modelar uno de las posibles relaciones presentes en el dataset, la red de retweets.\n",
    "\n",
    "#### Construcción de la edgelist\n",
    "\n",
    "Lo primero se os pide es que generéis la red. Hay diversas maneras de representar una red compleja, por ejemplo, si estuvierais interesados en trabajar en ellas desde el punto de vista teórico, la manera más habitual de representarlas es utilizando una [matriz de adyacencia](https://es.wikipedia.org/wiki/Matriz_de_adyacencia). En esta práctica vamos a centrarnos en el aspecto computacional, una de las maneras de más eficientes (computacionalmente hablando) de representar una red es mediante su [*edge list*](https://en.wikipedia.org/wiki/Edge_list), una tabla que especifica la relación a parejas entre las entidades.\n",
    "\n",
    "Las relaciones pueden ser bidireccionales o direccionales y tener algún peso asignado o no (weighted or unweighted). En el caso que nos ocupa, estamos hablando de una red dirigida, un usuario retuitea a otro, y podemos pensarla teniendo en cuenta cuántas veces esto ha pasado.\n",
    "\n",
    "#### Centralidad de grado\n",
    "\n",
    "Uno de los descriptores más comunes en el análisis de redes es el grado. El grado cuantifica cuántas aristas están conectadas a cada vértice~s~. En el caso de redes dirigidas como la que acabamos de crear este descriptor está descompuesto en el:\n",
    "- **in degree**: cuantas aristas apuntan al nodo\n",
    "- **out degree**: cuantas aristas salen del nodo\n",
    "\n",
    "Si haces un ranquing de estos valores vais a obtener una medida de centralidad, la [centralidad de grado](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), de cada uno de los nodos.\n",
    "\n",
    "### **Ejercicio 9**: Análisis de Interacciones de Retweets y Grados de Usuario (*0.75 puntos*)\n",
    "\n",
    "A partir de una muestra homogénea del 1% de los tweets, con la semilla 42 para garantizar la reproducibilidad, realiza un análisis de las interacciones de retweets entre usuarios en la red social.\n",
    "\n",
    "**Esquema**\n",
    "```Python\n",
    "seed = 42\n",
    "sample = tweets.<FILL IN>\n",
    "```\n",
    "Crea una tabla ```edgelist``` con la siguiente información:\n",
    "- ***src:*** usuario que retuitea\n",
    "- ***dst:*** usuario que es retuiteado\n",
    "- ***weight:*** número de veces que un usuario retuitea a otro.\n",
    "\n",
    "Filtrar el resultado para que contenga sólo las relaciones con un weight igual o mayor a dos.\n",
    "\n",
    "A continuación, genera una tabla `outDegree` con la información:\n",
    "- ***screen_name:*** nombre del usuario.\n",
    "- ***outDegree:*** out degree del nodo.\n",
    "\n",
    "Ordenado la tabla por out degree en orden descendente.\n",
    "\n",
    "Se os pide ahora que generéis una tabla `inDegree` con la información:\n",
    "- ***screen_name:*** nombre del usuario.\n",
    "- ***inDegree:*** in degree del nodo.\n",
    "\n",
    "Ordenad la tabla por in degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "tU-MEdEI6wns",
    "nbgrader": {
     "checksum": "6affa8ef55a904a3aa2d9fba078e7523",
     "grade": false,
     "grade_id": "cell-9ca4100b07adaa17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1J9f7A0l6wns",
    "nbgrader": {
     "checksum": "5f6aeec9d369b8308e6d17eb741bd40b",
     "grade": true,
     "grade_id": "cell-d9db82068d6e7c78",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hk6u1twM6wnt",
    "nbgrader": {
     "checksum": "db61ae2469a15f807ee873c532cbe2a9",
     "grade": false,
     "grade_id": "cell-06078f54727e9fbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### **Ejercicio 10**: Distribución del Grado de Salida en una Red de Retweets (*0.75 puntos*)\n",
    "\n",
    "A partir de una muestra del 1% de los tweets, con una semilla de 42 para asegurar la reproducibilidad, realiza un análisis básico de la red de retweets. Tu objetivo es calcular y mostrar la distribución de grados de los usuarios en la red de retweets.\n",
    "\n",
    "Para ello, sigue estos pasos:\n",
    "\n",
    "- Crea una tabla de Edgelist: Define una tabla que contenga las relaciones de retweet entre usuarios, donde cada fila representa un retweet realizado de un usuario a otro.\n",
    "\n",
    "- Calcula el Grado de Salida (Out-Degree): Determina cuántos retweets ha realizado cada usuario (es decir, el número de usuarios a los que cada usuario ha retweeteado).\n",
    "\n",
    "- Obtén la Distribución de Grado de Salida: Crea una tabla que muestre cuántos usuarios tienen un determinado número de retweets realizados. Ordena los resultados por el grado de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "zr8cAqg96wnt",
    "nbgrader": {
     "checksum": "9794540f4193e7d8f6ada110a6bc8c2d",
     "grade": false,
     "grade_id": "cell-28199afef1d47be1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "z4MWhVtp6wnt",
    "nbgrader": {
     "checksum": "c012c18b354c13e48abbac0609d35fa9",
     "grade": true,
     "grade_id": "cell-f6ae567623917fa0",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
