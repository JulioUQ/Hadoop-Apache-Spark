# Hadoop & Apache Spark — Fundamentos de Big Data

Este repositorio contiene materiales teóricos y prácticos del módulo **Big Data Fundamentals**, centrado en el análisis de datos masivos con **Apache Hadoop** y **Apache Spark**.

## Descripción del módulo

Las herramientas y tecnologías para el análisis de datos masivos (**Big Data**) están en constante evolución. El objetivo de este módulo no es solo enumerarlas, sino **proporcionar las bases teóricas necesarias** para comprender este ecosistema y poder analizar de forma autónoma las diferentes alternativas existentes.

A lo largo del módulo se abordan tres grandes bloques:

1. **Modelos de programación en entornos de datos masivos**

   * Introducción al modelo **MapReduce** y su implementación en **Apache Hadoop**.
   * Ventajas, limitaciones y principales componentes del ecosistema Hadoop.

2. **Procesamiento distribuido con Apache Spark**

   * Análisis de su arquitectura, funcionamiento interno y diferencias respecto a MapReduce.
   * Programación con **RDDs** y **DataFrames** usando **PySpark**.
   * Aplicaciones prácticas en el tratamiento de datos estructurados y no estructurados.

3. **Arquitecturas de procesamiento de datos masivos**

   * Introducción a la **arquitectura Lambda** como enfoque integral de procesamiento por lotes.
   * Breve revisión de arquitecturas basadas en **GPU** y su aplicación en Big Data.

## Objetivos y competencias

### Del módulo

* Comprender los **modelos de procesamiento distribuido** y sus aplicaciones.
* Conocer los principales **frameworks de Big Data**, como **Hadoop** y **Spark**.
* Familiarizarse con las **herramientas y servicios** para el procesamiento distribuido.
* Entender la **arquitectura Lambda** y su papel en el procesamiento por lotes.

### De la parte práctica

* Profundizar en el **paradigma Big Data**.
* Trabajar con datos en entornos **Apache Spark**.
* Aplicar el modelo **MapReduce** y su equivalencia en Spark.
* Usar **RDDs**, **PairRDDs** y la **API PySpark** para análisis y modelado de datos.

## Contenido del repositorio

HADOOP-APACHE-SPARK/
│
├── DataCamp Big Data Fundamentals with PySpark/
│   ├── Actividad_Batch_ES.ipynb
│   ├── Big Data Fundamentals with PySpark.md
│   ├── certificate.pdf
│   ├── PySpark_RDD_Cheat_Sheet.pdf
│   └── README.md
│
├── PEC 2/
│   └── credentials.md
│
├── Teoría y videos/
│   ├── Gestores de recursos.pdf
│   ├── HadoopDefinitiveGuide.pdf
│   ├── Sistemas de procesamiento distribuido en ...pdf
│   ├── Tunneling_SSH.pdf
│   ├── Uso de dataframes con Apache Spark.mp4
│   └── Uso de RDDs con Apache Spark.mp4
│
└── README.md

## Recursos de referencia

[RDD Programming Guide](https://archive.apache.org/dist/spark/docs/2.4.0/rdd-programming-guide.html)
[Spark SQL, DataFrames and Datasets Guide](https://archive.apache.org/dist/spark/docs/2.4.0/sql-getting-started.html)

## Certificación

Este repositorio incluye el certificado de finalización del curso **Big Data Fundamentals with PySpark** realizado en **DataCamp**, enfocado en:

* Transformaciones y acciones con **RDDs**
* Consultas con **SparkSQL**
* Modelado con **MLlib**
* Construcción de un recomendador de películas y un filtro de spam

## Autor

**Julio Úbeda Quesada**
[LinkedIn](https://www.linkedin.com/in/tu-perfil/)


